# -*- coding: utf-8 -*-
"""Forest Fires

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16cnQzg5p5ZvAFid_htFUuibGPwGf8ghs
"""

# Commented out IPython magic to ensure Python compatibility.
# Importing necessary libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier
# %matplotlib inline
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score

# Importing the dataset 
df = pd.read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/forest-fires/forestfires.csv")

"""PREPROCESSING & VISUALIZATION

"""

#display dataframe
df.head()

# checking for missing values (no null values)
df.isnull().sum()

# Checking for NA values (no NA values)
df.isna().sum()

# summary statistics
df.describe()

# checking correlation matrix
df.corr()

sns.pairplot(df)
plt.title('Pairplot of Attributes in Fire Damage Dataframe')

# Correlation Matrix looking at correlation between attributes
corr = df.corr()
mask = np.triu(np.ones_like(corr, dtype=bool))
f, ax = plt.subplots(figsize=(11, 9))
cmap = sns.diverging_palette(230, 20, as_cmap=True)
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,square=True, linewidths=.5, cbar_kws={"shrink": .5}, annot=True)
plt.title('Correlation Matrix of Numerical Features')

# Exploring the target variable 'area'
plt.figure(figsize=(15,6))
sns.distplot(df['area'], kde = True, rug = True)
plt.title('Density Plot for Area')

# conducting a univariate analysis for skew and kurtosis values to look at the distribution of the target variable 
print(df['area'].skew())
print(df['area'].kurtosis())

# looking into outliers for the target variable 
sns.boxplot(x = 'area', data = df)
plt.title("Boxplot for Area")

plt.figure(figsize=(40,20))

sns.boxplot(x=df['area'], y=df['month'], orient="h")
plt.title("Boxplot for Area by Month")

plt.figure(figsize=(15,6))
sns.countplot(df['month'])
plt.title("Number of Fires by Month")

plt.figure(figsize=(15,6))
sns.countplot(df['day'])
plt.title("Number of Fires by Day")

# grouping average area of forest fires by month (may, july, sep were highest, and jan, nov, mar were lowest)
df.groupby('month').mean()['area']

# grouping average area of forest fires by day (sat was highest)
df.groupby('day').mean()['area']

# looking at max area burnt by month 
df.groupby('month').max()['area']

# looking at max area burnt by day (max was sat in line with avg)
df.groupby('day').max()['area']

"""**Section 2: Transformations**"""

# converting day and month column to numerical values
daymap = {'mon': 0, 'tue': 1, 'wed': 2, 'thu': 3, 'fri': 4, 'sat': 5, 'sun': 6}
df['day'] = df['day'].map(daymap)

monthmap = {'jan': 0, 'feb': 1, 'mar': 2,'apr': 3,'may': 4,'jun':5,'jul': 6,'aug': 7,'sep': 8,'oct': 9,'nov': 10,'dec': 11}
df['month'] = df['month'].map(monthmap)

# converting days and months to cyclical values
import math

df["month_norm"] = 2 * math.pi * df["month"] / df["month"].max()

df["month_cos"] = np.cos(df["month_norm"])
df["month_sin"] = np.sin(df["month_norm"])

df["day_norm"] = 2 * math.pi * df["day"] / df["day"].max()

df["day_cos"] = np.cos(df["day_norm"])
df["day_sin"] = np.sin(df["day_norm"])

del df['month_norm']
del df['day_norm']
df.head()

#Duplicate dataset
df2 = df.copy()
df3 = df.copy()

#discretize the continuous variable
df2['area_bins'] = pd.cut(df2['area'], bins=[0,5, 10, 50, 100, 1100], include_lowest=True, 
                                 labels=['0-5', '5-10', '10-50', '50-100', '>100'])
df2.area_bins.value_counts()

# barplot comparing area bins
plt.figure(figsize=(15,5))
sns.countplot(data=df2, x="area_bins")
plt.title("Barplot Comparing Area Bins ")

# histogram plots for features
df2.hist(bins=50, figsize=(15,10), ec='w',)

#countplot for months
plt.figure(figsize=(15,6))
sns.countplot(df['month'])
plt.title('Counts of Months')

#countplot for days
plt.figure(figsize=(15,6))
sns.countplot(df['day'])
plt.title('Counts of Days')

#Oulier Detection

#copy dataframe to run outlier detection
dfo = df.copy()

for col in ['month', 'day']:
    dfo[col] = pd.Categorical(dfo[col])
    dfo[col] = dfo[col].cat.codes

#specify the 12 metrics column names to be modelled
model_columns=dfo.columns[:13]

#import library for isolation forests
from sklearn.ensemble import IsolationForest

clf=IsolationForest(contamination = 0.1)
clf.fit(dfo[model_columns])
pred = clf.predict(dfo[model_columns])
dfo['anomaly_isolation_forest']=pred
outliers=df.loc[dfo['anomaly_isolation_forest']==-1]
outlier_index=list(outliers.index)
#print(outlier_index)
#Find the number of anomalies and normal points here points classified -1 are anomalous
print(dfo['anomaly_isolation_forest'].value_counts())

# plotting outliers in 2d

plt.figure(figsize=(15,10))
#import PCA library to reduce dimensions
from sklearn.decomposition import PCA
pca = PCA(2)
pca.fit(dfo[model_columns])
res=pd.DataFrame(pca.transform(dfo[model_columns]))
Z = np.array(res)
plt.title("IsolationForest")
plt.contourf( Z, cmap=plt.cm.Blues_r)
b1 = plt.scatter(res[0], res[1], c='green',
                 s=20,label="normal points")
b1 =plt.scatter(res.iloc[outlier_index,0],res.iloc[outlier_index,1], c='red',s=20, marker="x", label="predicted outliers")
plt.legend(loc="upper right")
plt.show()

# plotting outliers in 3d
#import libraries
from sklearn.preprocessing import StandardScaler
from mpl_toolkits.mplot3d import Axes3D


pca = PCA(n_components=3)  # Reduce to k=3 dimensions
scaler = StandardScaler()
#normalize the metrics
X = scaler.fit_transform(dfo[model_columns])
X_reduce = pca.fit_transform(X)
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.set_zlabel("x_composite_3")
# Plot the compressed data points
ax.scatter(X_reduce[:, 0], X_reduce[:, 1], zs=X_reduce[:, 2], s=4, lw=1, label="inliers",c="green")
# Plot x's for the ground truth outliers
ax.scatter(X_reduce[outlier_index,0],X_reduce[outlier_index,1], X_reduce[outlier_index,2],
           lw=2, s=60, marker="x", c="red", label="outliers")
ax.legend()
plt.show()

#outlier method 2-Elliptical Envelope 
from sklearn.covariance import EllipticEnvelope
eclf=EllipticEnvelope(contamination=.1)
eclf.fit(dfo)
pred = eclf.predict(dfo)
dfo['anomaly_elliptic']=pred
outliers=dfo.loc[dfo['anomaly_elliptic']==-1]
outlier_index1=list(outliers.index)
#print(outlier_index)
#Find the number of anomalies and normal points here points classified -1 are anomalous
print(dfo['anomaly_elliptic'].value_counts())
print(dfo.head())

pca = PCA(2)
pca.fit(dfo[model_columns])
res=pd.DataFrame(pca.transform(dfo[model_columns]))
Z = np.array(res)
plt.title("Elliptic Envelopes")
plt.contourf( Z, cmap=plt.cm.Blues_r)
b1 = plt.scatter(res[0], res[1], c='green',
                 s=20,label="normal points")
b1 =plt.scatter(res.iloc[outlier_index1,0],res.iloc[outlier_index1,1], c='red',s=20,  marker="x",label="predicted outliers")
plt.legend(loc="upper right")
plt.show()

pca = PCA(n_components=3)  # Reduce to k=3 dimensions
scaler = StandardScaler()
#normalize the metrics
X = scaler.fit_transform(dfo[model_columns])
X_reduce = pca.fit_transform(X)
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.set_zlabel("x_composite_3")
# Plot the compressed data points
ax.scatter(X_reduce[:, 0], X_reduce[:, 1], zs=X_reduce[:, 2], s=4, lw=1, label="inliers",c="green")
# Plot x's for the ground truth outliers
ax.scatter(X_reduce[outlier_index1,0],X_reduce[outlier_index1,1], X_reduce[outlier_index1,2],
           lw=2, s=60, marker="x", c="red", label="outliers")
ax.legend()
plt.show()

# create a list of our conditions
conditions = [
    (dfo['anomaly_isolation_forest'] == -1) & (dfo['anomaly_elliptic'] == -1),
    (dfo['anomaly_isolation_forest'] == 1) & (dfo['anomaly_elliptic'] == 1),
    (dfo['anomaly_isolation_forest'] == -1) & (dfo['anomaly_elliptic'] == 1),
    (dfo['anomaly_isolation_forest'] == 1) & (dfo['anomaly_elliptic'] == -1)
    ]

# create a list of the values we want to assign for each condition
values = [-1, 1, 1, 1]

# create a new column and use np.select to assign values to it using our lists as arguments
dfo['outlier'] = np.select(conditions, values)

print(dfo['outlier'].value_counts())

del dfo['anomaly_isolation_forest']
del dfo['anomaly_elliptic']

print(dfo.head())

#process for classification

# converting day and month column to numerical values
area_class = {'0-5': 0, '10-50': 1, '5-10': 2, '50-100': 3, '>100': 4}
df2['area_bins'] = df2['area_bins'].map(area_class)

# x,y for classifcation
X=df2.drop(["area_bins", "month", "day","area"] ,axis=1)
y=df2.area_bins

#x,y for regression
Xl = df.drop(["month", "day","area"], axis=1)
yl = df.area

#split for regression

X_train, X_test, y_train, y_test = train_test_split(Xl, yl, test_size = 0.30)

#split for classification
X_trainc, X_testc, y_trainc, y_testc = train_test_split(X, y, test_size = 0.30)

"""**Statistical analysis for linear regression**

"""

#import libraries for statistical tests
import statsmodels.api as sm
from scipy.stats import zscore

#Regression after conducting log transformations
#columns that contain outliers
out_columns = ['area','FFMC','ISI','rain']
#describe the outlier columns
print(df3[out_columns].describe())

#applying log transformations
np.log1p(df3[out_columns]).skew(), np.log1p(df3[out_columns]).kurtosis()

#limiting the values to 3 standard deviations because the kurtosis is still high for FFMC
ztrans = df3.loc[:,['FFMC']].apply(zscore).abs() < 3

# convert the rain variable into a categorical variable since most the values are 0
df3['rain'] = df3['rain'].apply(lambda x: int(x > 0.0))

df3 = df3[ztrans.values]

#remove rain
out_columns.remove('rain')

#apply log again to outlier columns
df3[out_columns] = np.log1p(df3[out_columns])

df3[out_columns].skew()

# x and y values for regression analysis
X1 = df3.drop(columns=['area','month', 'day'])
y1 = df3['area']

X_trainlo, X_testlo, y_trainlo, y_testlo = train_test_split(X1, y1, test_size = 0.30)

#Linear Regressoin
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn import metrics


linear_reg = LinearRegression()
linear_reg.fit(X_train, y_train)

linear_pred = linear_reg.predict(X_test)


print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, linear_pred))  
print('Mean Squared Error:', metrics.mean_squared_error(y_test, linear_pred))  
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, linear_pred)))

#regression after applying log transformations to outlier variables

linear_reg = LinearRegression()
linear_reg.fit(X_trainlo, y_trainlo)

linear_pred = linear_reg.predict(X_testlo)


print('Mean Absolute Error:', metrics.mean_absolute_error(y_testlo, linear_pred))  
print('Mean Squared Error:', metrics.mean_squared_error(y_testlo, linear_pred))  
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_testlo, linear_pred)))

# Random Forest Regressor
from sklearn.ensemble import RandomForestRegressor

randomf_regression = RandomForestRegressor()
randomf_regression.fit(X_train, y_train)

randomf_pred = randomf_regression.predict(X_test)


print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, randomf_pred))  
print('Mean Squared Error:', metrics.mean_squared_error(y_test, randomf_pred))  
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, randomf_pred)))

"""Results of the regressor were not great, so will try some parameter tuning for Random Forest Regression"""

# random forest regressor with log transformed data
randomf_regression = RandomForestRegressor()
randomf_regression.fit(X_trainlo, y_trainlo)

randomf_pred = randomf_regression.predict(X_testlo)


print('Mean Absolute Error:', metrics.mean_absolute_error(y_testlo, randomf_pred))  
print('Mean Squared Error:', metrics.mean_squared_error(y_testlo, randomf_pred))  
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_testlo, randomf_pred)))

# Parameter tuning for Random Forest Regression
from sklearn.model_selection import GridSearchCV

parameter_grid = [{'bootstrap':[False,True],'n_estimators':[75,100,125,150,200], 'max_features':[1,2,4,6]}]
forest_reg = RandomForestRegressor()
rforest_search = GridSearchCV(forest_reg, parameter_grid, cv=5, scoring='neg_mean_squared_error', return_train_score=True)
rforest_search.fit(X_trainlo, y_trainlo)

#best parameter and estimator
rforest_search.best_params_ 
rforest_search.best_estimator_

# most important features
importances = rforest_search.best_estimator_.feature_importances_
feature_list = list(X1.columns)

feature_importance= sorted(zip(importances, feature_list), reverse=True)

df = pd.DataFrame(feature_importance, columns=['importance', 'feature'])
importance= list(df['importance'])
feature= list(df['feature'])

# Set the style
plt.style.use('bmh')
x_values = list(range(len(feature_importance)))

# bar graph for feature importance
plt.figure(figsize=(10,5))
plt.bar(x_values, importance, orientation = 'vertical')
plt.xticks(x_values, feature, rotation='vertical')
plt.ylabel('Importance'); plt.xlabel('Variable'); plt.title('Variable Importance');

# Predicting from log test data

final_model = rforest_search.best_estimator_
final_pred = final_model.predict(X_testlo)
final_mse = mean_squared_error(y_testlo, final_pred)
final_rmse = np.sqrt(final_mse)
final_mae = metrics.mean_absolute_error(y_testlo, final_pred)
print('The final RMSE on the test set is', round(final_rmse, 2))
print('The final MAE on the test set is', round(final_mse, 2))

# KNN Classification
#import classification report library
from sklearn.metrics import classification_report

knn = KNeighborsClassifier()
knn.fit(X_trainc, y_trainc)
knn_predi = knn.predict(X_testc)


print(classification_report(y_testc,knn_predi))

# Random Forest Classification
from sklearn.ensemble import RandomForestClassifier
forest = RandomForestClassifier()
forest.fit(X_trainc, y_trainc)
forest_predi = forest.predict(X_testc)
print(classification_report(y_testc,forest_predi))

# Oversampling to balance the classes of the outcome variable
# Example of oversampling a multi-class classification dataset
# import libraries for oversampling
from imblearn.over_sampling import SMOTE
from collections import Counter
from matplotlib import pyplot

oversample = SMOTE()
X_os, y_os = oversample.fit_resample(X,y)
counter = Counter(y_os)
for k,v in counter.items():
	per = v / len(y_os) * 100
	print('Class=%d, n=%d (%.3f%%)' % (k, v, per))
# plot the distribution
pyplot.bar(counter.keys(), counter.values())
pyplot.show()

#  nested cross-validation for random forest 
# import libraries for cross-validation and gridsearchcv
from numpy import mean
from numpy import std
from sklearn.model_selection import KFold
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
# configure the cross-validation 
cv_outer = KFold(n_splits=10, shuffle=True, random_state=1)
outer_results = list()
for train_ix, test_ix in cv_outer.split(X_os):
	# split data
	X_train, X_test = X_os[train_ix, :], X_os[test_ix, :]
	y_train, y_test = y_os[train_ix], y_os[test_ix]
	# configure the cross-validation procedure
	cv_inner = KFold(n_splits=3, shuffle=True, random_state=1)
	# random forest model
	model = RandomForestClassifier(random_state=1)
	# define parameters for tuning
	space = dict()
	space['n_estimators'] = [10, 100, 500]
	space['max_features'] = [2, 4, 6]
	# define search
	search = GridSearchCV(model, space, scoring='accuracy', cv=cv_inner, refit=True)
	result = search.fit(X_train, y_train)
	# best performing model
	best_model = result.best_estimator_
	# evaluate model 
	yhat = best_model.predict(X_test)
	# accuracy score 
	acc = accuracy_score(y_test, yhat)
	# store the result
	outer_results.append(acc)
	print('>acc=%.3f, est=%.3f, cfg=%s' % (acc, result.best_score_, result.best_params_))
# summarize the performance of the model
print('Accuracy: %.3f (%.3f)' % (mean(outer_results), std(outer_results)))

#print classication report for random forest model(oversampling)
print(classification_report(y_test, yhat))

#confusion matrix for random
f, ax = plt.subplots(figsize=(10, 6))

confusion_mx = confusion_matrix(y_test, yhat)
ax= plt.subplot()
sns.heatmap(confusion_mx, annot=True, ax = ax, cmap= 'Blues');
ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels');
plt.title('Confusion Matrix for Random Forest Model')

#features importance plot from random forest classification (oversampling)
importances = result.best_estimator_.feature_importances_
feature_list = list(X.columns)

feature_importance= sorted(zip(importances, feature_list), reverse=True)
df = pd.DataFrame(feature_importance, columns=['importance', 'feature'])
importance= list(df['importance'])
feature= list(df['feature'])

plt.style.use('bmh')
x_values = list(range(len(feature_importance)))

# Make a bar chart
plt.figure(figsize=(10,5))
plt.bar(x_values, importance, orientation = 'vertical')
# Tick labels for x axis
plt.xticks(x_values, feature, rotation='vertical')
# Axis labels and title
plt.ylabel('Importance'); plt.xlabel('Variable'); plt.title('Variable Importances');

# Running KFold Cross validation again with KNN
cv_outer = KFold(n_splits=10, shuffle=True, random_state=1)

# parameters for knn
pr = dict()
pr['n_neighbors'] = list(range(1,30))
pr['weights'] = ['uniform','distance']
param_grid = [{'n_neighbors': list(range(1,30)),
             'weights': ['uniform','distance']}]
# enumerate splits
outer_results = list()
for train_ix, test_ix in cv_outer.split(X_os):
	X_train, X_test = X_os[train_ix, :], X_os[test_ix, :]
	y_train, y_test = y_os[train_ix], y_os[test_ix]
	# configure the cross-validation procedure
	cv_inner = KFold(n_splits=3, shuffle=True, random_state=1)
	# knn model
	knn = KNeighborsClassifier()
  
	search = GridSearchCV(knn, pr, scoring='accuracy', cv=cv_inner, refit=True)
	result = search.fit(X_train, y_train)
	best_model = result.best_estimator_
	yhat = best_model.predict(X_test)
	acc = accuracy_score(y_test, yhat)
	outer_results.append(acc)
	# report progress
	print('>acc=%.3f, est=%.3f, cfg=%s' % (acc, result.best_score_, result.best_params_))
# summarize the estimated performance of the model
print('Accuracy: %.3f (%.3f)' % (mean(outer_results), std(outer_results)))

# Printing classification report for knn model (oversampling)
print(classification_report(y_test, yhat))

#Confusion matrix for knn model
f, ax = plt.subplots(figsize=(10, 6))
confusion_mx = confusion_matrix(y_test, yhat)
ax= plt.subplot()
sns.heatmap(confusion_mx, annot=True, ax = ax, cmap= 'Blues');
ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels');
plt.title("Confusion Matrix for K-NN Model")
